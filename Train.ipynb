{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f969f4b-71e9-4ca2-97f4-baf53138adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from ASTGCN.astgcn import ASTGCN, params\n",
    "from utility.utils import compute_chebyshev_polynomials, compute_scaled_laplacian, generate_adj_matrix, normalise_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0676b005-177c-4f83-9f5f-45f25487dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json', 'r') as f:\n",
    "    config_file = json.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5203396-2862-4030-bcd5-0597da585d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_verticies = int(config_file['num_of_vertices'])\n",
    "points_per_hour = int(config_file['points_per_hour'])\n",
    "\n",
    "num_for_predict = int(config_file['num_for_predict'])\n",
    "n_features = int(config_file['n_features'])\n",
    "n_history = int(config_file['n_history'])\n",
    "\n",
    "distance_matrix_path = str(config_file['distance_filename'])\n",
    "graph_signal_path = str(config_file['graph_signal_path'])\n",
    "\n",
    "epochs = int(config_file['epochs'])\n",
    "cheb_polynomial = int(config_file['cheb_polynomials'])\n",
    "lr = float(config_file['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf28d8d-9834-400f-8561-c53c532f0527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307, 307)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix = pd.read_csv(distance_matrix_path).to_numpy()\n",
    "adj_matrix = generate_adj_matrix(distance_matrix, n_verticies)\n",
    "adj_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7677908-34b2-408a-bf01-05584eb8dec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16992, 307, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_signal_matrix = np.load(graph_signal_path)['data']\n",
    "graph_signal_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c114e8-7d48-4762-8494-45e10ab4e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_graph_signal_matrix = normalise_data(graph_signal_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14a14e17-e847-4a0f-a225-2a1378aeb42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16,992 = 59 days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a89f9110-e8af-4271-921f-5c24fd9d8d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simple_dataset(data, num_hist=12, num_pred=12, train_ratio=0.7):\n",
    "    samples_per_day = 288  # 24 * 12\n",
    "    samples_per_week = 2016  # 7 * 288\n",
    "    \n",
    "    start_idx = samples_per_week + num_hist\n",
    "    \n",
    "    recent, daily, weekly, targets = [], [], [], []\n",
    "    \n",
    "    for i in range(start_idx, len(data) - num_pred):\n",
    "        recent.append(data[i-num_hist:i])\n",
    "        daily.append(data[i-samples_per_day-num_hist : i-samples_per_day])\n",
    "        weekly.append(data[i-samples_per_week-num_hist : i-samples_per_week])\n",
    "        targets.append(data[i:i+num_pred])\n",
    "    \n",
    "    n = len(recent)\n",
    "    split_idx = int(n * train_ratio)\n",
    "    \n",
    "    return {\n",
    "            'x_train_recent': np.array(recent[:split_idx]),\n",
    "            'x_train_daily': np.array(daily[:split_idx]),\n",
    "            'x_train_weekly': np.array(weekly[:split_idx]),\n",
    "            'y_train': np.array(targets[:split_idx]),\n",
    "            'x_test_recent': np.array(recent[split_idx:]),\n",
    "            'x_test_daily': np.array(daily[split_idx:]),\n",
    "            'x_test_weekly': np.array(weekly[split_idx:]),\n",
    "            'y_test': np.array(targets[split_idx:])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f5cb15b-6b47-420a-ae4c-e2d2adacbf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = generate_simple_dataset(normalised_graph_signal_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d758deaa-588d-43b6-a867-0778fce502f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10466, 12, 307, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['x_train_recent'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5577e88-f461-4c30-a0ab-97099a33e48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10466, 12, 307, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['x_train_daily'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f6987b0-17d0-4252-9f9a-fb24a7e68eaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10466, 12, 307, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['x_train_weekly'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bd8317d-a5d6-4c2c-9e2a-58c7ca856276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10466, 12, 307, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['y_train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e4606f0-b9fb-4623-ba3e-9d77a443e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x_recent, x_daily, x_weekly, output):\n",
    "\n",
    "        self.x_recent = torch.tensor(x_recent, dtype=torch.float32).permute(0, 2, 3, 1) # x, T, N, F - > x, N, F, T\n",
    "        self.x_daily = torch.tensor(x_daily, dtype=torch.float32).permute(0, 2, 3, 1)\n",
    "        self.x_weekly = torch.tensor(x_weekly, dtype=torch.float32).permute(0, 2, 3, 1)\n",
    "        self.output = torch.tensor(output, dtype=torch.float32).permute(0, 2, 3, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return self.x_recent.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.x_recent[idx], self.x_daily[idx], self.x_weekly[idx], self.output[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff5850bc-24e4-4ca4-b81d-6bfda8ede326",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(\n",
    "    dataset['x_train_recent'][0:40*16], # only doing the first 40 batches\n",
    "    dataset['x_train_daily'][0:40*16], \n",
    "    dataset['x_train_weekly'][0:40*16],\n",
    "    dataset['y_train'][0:40*16]\n",
    ")\n",
    "\n",
    "test_dataset = CustomDataset(\n",
    "    dataset['x_test_recent'][0:40*16],\n",
    "    dataset['x_test_daily'][0:40*16],\n",
    "    dataset['x_test_weekly'][0:40*16],\n",
    "    dataset['y_test'][0:40*16]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5e50584-161b-4d22-829d-c0f7dcac5800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x108d95ef0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e0f880d-efb6-4484-bdbd-83bdb9098c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf6f0a0d-51c0-49c8-ba4c-5322d70d3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_scaled = compute_scaled_laplacian(adj_matrix)\n",
    "cheb_polynomials = compute_chebyshev_polynomials(L_scaled, cheb_polynomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "173b15f8-29d8-4c14-b9d9-c501038b0aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelParams = params (\n",
    "    n_timesteps = n_history,\n",
    "    cheb_polynomials = cheb_polynomials,\n",
    "    n_features = n_features,\n",
    "    n_verticies = n_verticies,\n",
    "    n_layers = 1,\n",
    "    lr = 4e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9513344e-8d3c-4880-a6f0-25fbc728f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ASTGCN(ModelParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d064c6b-b316-4961-bbad-be06cd851f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Epoch: 1\n",
      "Train loss: 49.40409529209137\n",
      "Test loss: 37.22625911235809\n",
      "==================================================\n",
      "==================================================\n",
      "Epoch: 2\n",
      "Train loss: 48.67265683412552\n",
      "Test loss: 36.482948780059814\n",
      "==================================================\n",
      "==================================================\n",
      "Epoch: 3\n",
      "Train loss: 47.15277272462845\n",
      "Test loss: 35.73092520236969\n",
      "==================================================\n",
      "==================================================\n",
      "Epoch: 4\n",
      "Train loss: 45.90956288576126\n",
      "Test loss: 37.406772911548615\n",
      "==================================================\n",
      "==================================================\n",
      "Epoch: 5\n",
      "Train loss: 45.60867887735367\n",
      "Test loss: 37.71971535682678\n",
      "==================================================\n",
      "==================================================\n",
      "Epoch: 6\n",
      "Train loss: 44.39013624191284\n",
      "Test loss: 37.76668471097946\n",
      "==================================================\n",
      "==================================================\n",
      "Epoch: 7\n",
      "Train loss: 44.00029045343399\n",
      "Test loss: 37.91232639551163\n",
      "==================================================\n",
      "==================================================\n",
      "Epoch: 8\n",
      "Train loss: 44.11412167549133\n",
      "Test loss: 36.265748262405396\n",
      "==================================================\n",
      "==================================================\n",
      "Epoch: 9\n",
      "Train loss: 43.92029392719269\n",
      "Test loss: 37.29431837797165\n",
      "==================================================\n",
      "==================================================\n",
      "Epoch: 10\n",
      "Train loss: 43.38738417625427\n",
      "Test loss: 37.79525923728943\n",
      "==================================================\n",
      "==================================================\n",
      "Epoch: 11\n",
      "Train loss: 43.28458559513092\n",
      "Test loss: 37.12257742881775\n",
      "==================================================\n",
      "==================================================\n",
      "Epoch: 12\n",
      "Train loss: 43.210021674633026\n",
      "Test loss: 37.235741436481476\n",
      "==================================================\n",
      "==================================================\n",
      "Epoch: 13\n",
      "Train loss: 43.18858712911606\n",
      "Test loss: 37.14897072315216\n",
      "==================================================\n",
      "==================================================\n",
      "Epoch: 14\n",
      "Train loss: 43.1192507147789\n",
      "Test loss: 37.10729646682739\n",
      "==================================================\n",
      "==================================================\n",
      "Epoch: 15\n",
      "Train loss: 43.25756078958511\n",
      "Test loss: 37.68697863817215\n",
      "==================================================\n",
      "==================================================\n",
      "Epoch: 16\n",
      "Train loss: 43.20125073194504\n",
      "Test loss: 37.23243671655655\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x_recent, x_daily, x_weekly, output \u001b[38;5;129;01min\u001b[39;00m test_dataloader:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m         pred_out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_recent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_daily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_weekly\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m         loss = model.compute_loss(pred_out, output)\n\u001b[32m     21\u001b[39m         test_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TModel/ASTGCN/astgcn.py:93\u001b[39m, in \u001b[36mASTGCN.forward\u001b[39m\u001b[34m(self, x_recent, x_daily_p, x_weekly)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_recent, layer_daily, layer_weekly \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.blocks[\u001b[33m\"\u001b[39m\u001b[33mrecent\u001b[39m\u001b[33m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m.blocks[\u001b[33m\"\u001b[39m\u001b[33mdaily\u001b[39m\u001b[33m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m.blocks[\u001b[33m\"\u001b[39m\u001b[33mweekly\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m     92\u001b[39m     x_recent = layer_recent(x_recent)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     x_daily = \u001b[43mlayer_daily\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_daily_p\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m     x_weekly = layer_weekly(x_weekly)\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m#x_recent = self.relu(self.linear_recent(x_recent))\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m#x_daily_p = self.relu(self.linear_daily(x_daily))\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m#x_weekly = self.relu(self.linear_weekly(x_weekly))\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TModel/ASTGCN/blocks/STblock.py:41\u001b[39m, in \u001b[36mSTBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     39\u001b[39m spatial_attn = \u001b[38;5;28mself\u001b[39m.spatial_attn.forward(x) \u001b[38;5;66;03m# (B,N,N)\u001b[39;00m\n\u001b[32m     40\u001b[39m temporal_attn = \u001b[38;5;28mself\u001b[39m.temporal_attn.forward(x) \u001b[38;5;66;03m# (B,T,T)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m spatial_conv = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspatial_conv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_attn\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B,N,O,T)\u001b[39;00m\n\u001b[32m     42\u001b[39m output = \u001b[38;5;28mself\u001b[39m.temporal_conv.forward(spatial_conv, temporal_attn) \u001b[38;5;66;03m# (B,N,F,T)\u001b[39;00m\n\u001b[32m     44\u001b[39m output = output.permute(\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m3\u001b[39m,\u001b[32m2\u001b[39m) \u001b[38;5;66;03m# swap F and T  ... T, F)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TModel/ASTGCN/blocks/convblock.py:88\u001b[39m, in \u001b[36mSpatialConv.forward\u001b[39m\u001b[34m(self, x, spatial_attn)\u001b[39m\n\u001b[32m     84\u001b[39m     theta_k = \u001b[38;5;28mself\u001b[39m.weight[k] \u001b[38;5;66;03m# (F, O)\u001b[39;00m\n\u001b[32m     86\u001b[39m     Pk_attn = Pk.unsqueeze(\u001b[32m0\u001b[39m) * spatial_attn_expanded \u001b[38;5;66;03m# (1, N, N) (B*T, N, N) -> (B*T, N, N)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     aggregated = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPk_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_reshaped\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B*T, N, N) (B*T, N, F) -> (B*T, N, F)\u001b[39;00m\n\u001b[32m     90\u001b[39m     outputs += torch.matmul(aggregated, theta_k) \u001b[38;5;66;03m# (B*T, N, F) (F, O) -> (B*T, N, O)\u001b[39;00m\n\u001b[32m     92\u001b[39m outputs = outputs.view(batch_size, num_timesteps, num_nodes, \u001b[38;5;28mself\u001b[39m.out_features) \u001b[38;5;66;03m# (B*T, N, O) -> (B, T, N, O)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    for x_recent, x_daily, x_weekly, output in train_dataloader:\n",
    "\n",
    "        model.optimizer.zero_grad()\n",
    "        pred_out = model(x_recent, x_daily, x_weekly)\n",
    "        loss = model.compute_loss(pred_out, output)\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    for x_recent, x_daily, x_weekly, output in test_dataloader:\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            pred_out = model(x_recent, x_daily, x_weekly)\n",
    "            loss = model.compute_loss(pred_out, output)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "    print(\"=\"*50)\n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    print(f\"Train loss: {train_loss}\")\n",
    "    print(f\"Test loss: {test_loss}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d44056f-b683-40c8-a2c8-0e5ca4f411b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASTGCN(\n",
       "  (blocks): ModuleDict(\n",
       "    (recent): ModuleList(\n",
       "      (0): STBlock(\n",
       "        (spatial_attn): SpatialAttention(\n",
       "          (sigmoid): Sigmoid()\n",
       "          (softmax): Softmax(dim=None)\n",
       "        )\n",
       "        (temporal_attn): TemporalAttention(\n",
       "          (sigmoid): Sigmoid()\n",
       "          (softmax): Softmax(dim=None)\n",
       "        )\n",
       "        (spatial_conv): SpatialConv(\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (temporal_conv): TemporalConv(\n",
       "          (conv): Conv2d(3, 3, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (ln): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (gelu): GELU(approximate='none')\n",
       "      )\n",
       "      (1): Linear(in_features=12, out_features=12, bias=False)\n",
       "    )\n",
       "    (daily): ModuleList(\n",
       "      (0): STBlock(\n",
       "        (spatial_attn): SpatialAttention(\n",
       "          (sigmoid): Sigmoid()\n",
       "          (softmax): Softmax(dim=None)\n",
       "        )\n",
       "        (temporal_attn): TemporalAttention(\n",
       "          (sigmoid): Sigmoid()\n",
       "          (softmax): Softmax(dim=None)\n",
       "        )\n",
       "        (spatial_conv): SpatialConv(\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (temporal_conv): TemporalConv(\n",
       "          (conv): Conv2d(3, 3, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (ln): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (gelu): GELU(approximate='none')\n",
       "      )\n",
       "      (1): Linear(in_features=12, out_features=12, bias=False)\n",
       "    )\n",
       "    (weekly): ModuleList(\n",
       "      (0): STBlock(\n",
       "        (spatial_attn): SpatialAttention(\n",
       "          (sigmoid): Sigmoid()\n",
       "          (softmax): Softmax(dim=None)\n",
       "        )\n",
       "        (temporal_attn): TemporalAttention(\n",
       "          (sigmoid): Sigmoid()\n",
       "          (softmax): Softmax(dim=None)\n",
       "        )\n",
       "        (spatial_conv): SpatialConv(\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (temporal_conv): TemporalConv(\n",
       "          (conv): Conv2d(3, 3, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (ln): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (gelu): GELU(approximate='none')\n",
       "      )\n",
       "      (1): Linear(in_features=12, out_features=12, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (linear_recent): Linear(in_features=12, out_features=12, bias=False)\n",
       "  (linear_daily_p): Linear(in_features=12, out_features=12, bias=False)\n",
       "  (linear_weekly): Linear(in_features=12, out_features=12, bias=False)\n",
       "  (relu): ReLU()\n",
       "  (loss): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8457f5-8439-40ef-95a4-04b4edc0f944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
